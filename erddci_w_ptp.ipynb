{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Copyright 2022 Google LLC. Double-click for license information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Prompt-to-Prompt with Stable Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Union, Tuple, List, Callable, Dict\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "import torch.nn.functional as nnf\n",
    "import numpy as np\n",
    "import abc\n",
    "import ptp_utils\n",
    "import seq_aligner\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For loading the Stable Diffusion using Diffusers, follow the instuctions https://huggingface.co/blog/stable_diffusion and update ```MY_TOKEN``` with your token.\n",
    "Set ```LOW_RESOURCE``` to ```True``` for running on 12GB GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
    "# MY_TOKEN = ''\n",
    "LOW_RESOURCE = False \n",
    "NUM_DIFFUSION_STEPS = 50\n",
    "GUIDANCE_SCALE = 7.5\n",
    "MAX_NUM_WORDS = 77\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "ldm_stable = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", local_files_only=True, scheduler=scheduler).to(device)\n",
    "try:\n",
    "    ldm_stable.disable_xformers_memory_efficient_attention()\n",
    "except AttributeError:\n",
    "    print(\"Attribute disable_xformers_memory_efficient_attention() is missing\")\n",
    "tokenizer = ldm_stable.tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Prompt-to-Prompt Attnetion Controllers\n",
    "Our main logic is implemented in the `forward` call in an `AttentionControl` object.\n",
    "The forward is called in each attention layer of the diffusion model and it can modify the input attnetion weights `attn`.\n",
    "\n",
    "`is_cross`, `place_in_unet in (\"down\", \"mid\", \"up\")`, `AttentionControl.cur_step` help us track the exact attention layer and timestamp during the diffusion iference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalBlend:\n",
    "\n",
    "    def __call__(self, x_t, attention_store):\n",
    "        k = 1\n",
    "        maps = attention_store[\"down_cross\"][2:4] + attention_store[\"up_cross\"][:3]\n",
    "        maps = [item.reshape(self.alpha_layers.shape[0], -1, 1, 16, 16, MAX_NUM_WORDS) for item in maps]\n",
    "        maps = torch.cat(maps, dim=1)\n",
    "        maps = (maps * self.alpha_layers).sum(-1).mean(1)\n",
    "        mask = nnf.max_pool2d(maps, (k * 2 + 1, k * 2 +1), (1, 1), padding=(k, k))\n",
    "        mask = nnf.interpolate(mask, size=(x_t.shape[2:]))\n",
    "        mask = mask / mask.max(2, keepdims=True)[0].max(3, keepdims=True)[0]\n",
    "        mask = mask.gt(self.threshold)\n",
    "        mask = (mask[:1] + mask[1:]).float()\n",
    "        x_t = x_t[:1] + mask * (x_t - x_t[:1])\n",
    "        return x_t\n",
    "       \n",
    "    def __init__(self, prompts: List[str], words: [List[List[str]]], threshold=.3):\n",
    "        alpha_layers = torch.zeros(len(prompts),  1, 1, 1, 1, MAX_NUM_WORDS)\n",
    "        for i, (prompt, words_) in enumerate(zip(prompts, words)):\n",
    "            if type(words_) is str:\n",
    "                words_ = [words_]\n",
    "            for word in words_:\n",
    "                ind = ptp_utils.get_word_inds(prompt, word, tokenizer)\n",
    "                alpha_layers[i, :, :, :, :, ind] = 1\n",
    "        self.alpha_layers = alpha_layers.to(device)\n",
    "        self.threshold = threshold\n",
    "\n",
    "\n",
    "class AttentionControl(abc.ABC):\n",
    "    \n",
    "    def step_callback(self, x_t):\n",
    "        return x_t\n",
    "    \n",
    "    def between_steps(self):\n",
    "        return\n",
    "    \n",
    "    @property\n",
    "    def num_uncond_att_layers(self):\n",
    "        return self.num_att_layers if LOW_RESOURCE else 0\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def forward (self, attn, is_cross: bool, place_in_unet: str):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __call__(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        if self.cur_att_layer >= self.num_uncond_att_layers:\n",
    "            if LOW_RESOURCE:\n",
    "                attn = self.forward(attn, is_cross, place_in_unet)\n",
    "            else:\n",
    "                h = attn.shape[0]\n",
    "                attn[h // 2:] = self.forward(attn[h // 2:], is_cross, place_in_unet)\n",
    "        self.cur_att_layer += 1\n",
    "        if self.cur_att_layer == self.num_att_layers + self.num_uncond_att_layers:\n",
    "            self.cur_att_layer = 0\n",
    "            self.cur_step += 1\n",
    "            self.between_steps()\n",
    "        return attn\n",
    "    \n",
    "    def reset(self):\n",
    "        self.cur_step = 0\n",
    "        self.cur_att_layer = 0\n",
    "\n",
    "    def __init__(self):\n",
    "        self.cur_step = 0\n",
    "        self.num_att_layers = -1\n",
    "        self.cur_att_layer = 0\n",
    "\n",
    "class EmptyControl(AttentionControl):\n",
    "    \n",
    "    def forward (self, attn, is_cross: bool, place_in_unet: str):\n",
    "        return attn\n",
    "    \n",
    "    \n",
    "class AttentionStore(AttentionControl):\n",
    "\n",
    "    @staticmethod\n",
    "    def get_empty_store():\n",
    "        return {\"down_cross\": [], \"mid_cross\": [], \"up_cross\": [],\n",
    "                \"down_self\": [],  \"mid_self\": [],  \"up_self\": []}\n",
    "\n",
    "    def forward(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        key = f\"{place_in_unet}_{'cross' if is_cross else 'self'}\"\n",
    "        if attn.shape[1] <= 32 ** 2:  # avoid memory overhead\n",
    "            self.step_store[key].append(attn)\n",
    "        return attn\n",
    "\n",
    "    def between_steps(self):\n",
    "        if len(self.attention_store) == 0:\n",
    "            self.attention_store = self.step_store\n",
    "        else:\n",
    "            for key in self.attention_store:\n",
    "                for i in range(len(self.attention_store[key])):\n",
    "                    self.attention_store[key][i] += self.step_store[key][i]\n",
    "        self.step_store = self.get_empty_store()\n",
    "\n",
    "    def get_average_attention(self):\n",
    "        average_attention = {key: [item / self.cur_step for item in self.attention_store[key]] for key in self.attention_store}\n",
    "        return average_attention\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        super(AttentionStore, self).reset()\n",
    "        self.step_store = self.get_empty_store()\n",
    "        self.attention_store = {}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AttentionStore, self).__init__()\n",
    "        self.step_store = self.get_empty_store()\n",
    "        self.attention_store = {}\n",
    "\n",
    "        \n",
    "class AttentionControlEdit(AttentionStore, abc.ABC):\n",
    "    \n",
    "    def step_callback(self, x_t):\n",
    "        if self.local_blend is not None:\n",
    "            x_t = self.local_blend(x_t, self.attention_store)\n",
    "        return x_t\n",
    "        \n",
    "    def replace_self_attention(self, attn_base, att_replace):\n",
    "        if att_replace.shape[2] <= 16 ** 2:\n",
    "            return attn_base.unsqueeze(0).expand(att_replace.shape[0], *attn_base.shape)\n",
    "        else:\n",
    "            return att_replace\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def replace_cross_attention(self, attn_base, att_replace):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def forward(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        super(AttentionControlEdit, self).forward(attn, is_cross, place_in_unet)\n",
    "        if is_cross or (self.num_self_replace[0] <= self.cur_step < self.num_self_replace[1]):\n",
    "            h = attn.shape[0] // (self.batch_size)\n",
    "            attn = attn.reshape(self.batch_size, h, *attn.shape[1:])\n",
    "            attn_base, attn_repalce = attn[0], attn[1:]\n",
    "            if is_cross:\n",
    "                alpha_words = self.cross_replace_alpha[self.cur_step]\n",
    "                attn_repalce_new = self.replace_cross_attention(attn_base, attn_repalce) * alpha_words + (1 - alpha_words) * attn_repalce\n",
    "                attn[1:] = attn_repalce_new\n",
    "            else:\n",
    "                attn[1:] = self.replace_self_attention(attn_base, attn_repalce)\n",
    "            attn = attn.reshape(self.batch_size * h, *attn.shape[2:])\n",
    "        return attn\n",
    "    \n",
    "    def __init__(self, prompts, num_steps: int,\n",
    "                 cross_replace_steps: Union[float, Tuple[float, float], Dict[str, Tuple[float, float]]],\n",
    "                 self_replace_steps: Union[float, Tuple[float, float]],\n",
    "                 local_blend: Optional[LocalBlend]):\n",
    "        super(AttentionControlEdit, self).__init__()\n",
    "        self.batch_size = len(prompts)\n",
    "        self.cross_replace_alpha = ptp_utils.get_time_words_attention_alpha(prompts, num_steps, cross_replace_steps, tokenizer).to(device)\n",
    "        if type(self_replace_steps) is float:\n",
    "            self_replace_steps = 0, self_replace_steps\n",
    "        self.num_self_replace = int(num_steps * self_replace_steps[0]), int(num_steps * self_replace_steps[1])\n",
    "        self.local_blend = local_blend\n",
    "\n",
    "class AttentionReplace(AttentionControlEdit):\n",
    "\n",
    "    def replace_cross_attention(self, attn_base, att_replace):\n",
    "        return torch.einsum('hpw,bwn->bhpn', attn_base, self.mapper)\n",
    "      \n",
    "    def __init__(self, prompts, num_steps: int, cross_replace_steps: float, self_replace_steps: float,\n",
    "                 local_blend: Optional[LocalBlend] = None):\n",
    "        super(AttentionReplace, self).__init__(prompts, num_steps, cross_replace_steps, self_replace_steps, local_blend)\n",
    "        self.mapper = seq_aligner.get_replacement_mapper(prompts, tokenizer).to(device)\n",
    "        \n",
    "\n",
    "class AttentionRefine(AttentionControlEdit):\n",
    "\n",
    "    def replace_cross_attention(self, attn_base, att_replace):\n",
    "        attn_base_replace = attn_base[:, :, self.mapper].permute(2, 0, 1, 3)\n",
    "        attn_replace = attn_base_replace * self.alphas + att_replace * (1 - self.alphas)\n",
    "        return attn_replace\n",
    "\n",
    "    def __init__(self, prompts, num_steps: int, cross_replace_steps: float, self_replace_steps: float,\n",
    "                 local_blend: Optional[LocalBlend] = None):\n",
    "        super(AttentionRefine, self).__init__(prompts, num_steps, cross_replace_steps, self_replace_steps, local_blend)\n",
    "        self.mapper, alphas = seq_aligner.get_refinement_mapper(prompts, tokenizer)\n",
    "        self.mapper, alphas = self.mapper.to(device), alphas.to(device)\n",
    "        self.alphas = alphas.reshape(alphas.shape[0], 1, 1, alphas.shape[1])\n",
    "\n",
    "class AttentionReweight(AttentionControlEdit):\n",
    "\n",
    "    def replace_cross_attention(self, attn_base, att_replace):\n",
    "        if self.prev_controller is not None:\n",
    "            attn_base = self.prev_controller.replace_cross_attention(attn_base, att_replace)\n",
    "        attn_replace = attn_base[None, :, :, :] * self.equalizer[:, None, None, :]\n",
    "        return attn_replace\n",
    "\n",
    "    def __init__(self, prompts, num_steps: int, cross_replace_steps: float, self_replace_steps: float, equalizer,\n",
    "                local_blend: Optional[LocalBlend] = None, controller: Optional[AttentionControlEdit] = None):\n",
    "        super(AttentionReweight, self).__init__(prompts, num_steps, cross_replace_steps, self_replace_steps, local_blend)\n",
    "        self.equalizer = equalizer.to(device)\n",
    "        self.prev_controller = controller\n",
    "\n",
    "\n",
    "def get_equalizer(text: str, word_select: Union[int, Tuple[int, ...]], values: Union[List[float],\n",
    "                  Tuple[float, ...]]):\n",
    "    if type(word_select) is int or type(word_select) is str:\n",
    "        word_select = (word_select,)\n",
    "    equalizer = torch.ones(len(values), 77)\n",
    "    values = torch.tensor(values, dtype=torch.float32)\n",
    "    for word in word_select:\n",
    "        inds = ptp_utils.get_word_inds(text, word, tokenizer)\n",
    "        equalizer[:, inds] = values\n",
    "    return equalizer\n",
    "\n",
    "def aggregate_attention(attention_store: AttentionStore, res: int, from_where: List[str], is_cross: bool, select: int):\n",
    "    out = []\n",
    "    attention_maps = attention_store.get_average_attention()\n",
    "    num_pixels = res ** 2\n",
    "    for location in from_where:\n",
    "        for item in attention_maps[f\"{location}_{'cross' if is_cross else 'self'}\"]:\n",
    "            if item.shape[1] == num_pixels:\n",
    "                cross_maps = item.reshape(len(prompts), -1, res, res, item.shape[-1])[select]\n",
    "                out.append(cross_maps)\n",
    "    out = torch.cat(out, dim=0)\n",
    "    out = out.sum(0) / out.shape[0]\n",
    "    return out.cpu()\n",
    "\n",
    "\n",
    "def show_cross_attention(attention_store: AttentionStore, res: int, from_where: List[str], select: int = 0):\n",
    "    tokens = tokenizer.encode(prompts[select])\n",
    "    decoder = tokenizer.decode\n",
    "    attention_maps = aggregate_attention(attention_store, res, from_where, True, select)\n",
    "    images = []\n",
    "    for i in range(len(tokens)):\n",
    "        image = attention_maps[:, :, i]\n",
    "        image = 255 * image / image.max()\n",
    "        image = image.unsqueeze(-1).expand(*image.shape, 3)\n",
    "        image = image.numpy().astype(np.uint8)\n",
    "        image = np.array(Image.fromarray(image).resize((256, 256)))\n",
    "        image = ptp_utils.text_under_image(image, decoder(int(tokens[i])))\n",
    "        images.append(image)\n",
    "    ptp_utils.view_images(np.stack(images, axis=0))\n",
    "    \n",
    "\n",
    "def show_self_attention_comp(attention_store: AttentionStore, res: int, from_where: List[str],\n",
    "                        max_com=10, select: int = 0):\n",
    "    attention_maps = aggregate_attention(attention_store, res, from_where, False, select).numpy().reshape((res ** 2, res ** 2))\n",
    "    u, s, vh = np.linalg.svd(attention_maps - np.mean(attention_maps, axis=1, keepdims=True))\n",
    "    images = []\n",
    "    for i in range(max_com):\n",
    "        image = vh[i].reshape(res, res)\n",
    "        image = image - image.min()\n",
    "        image = 255 * image / image.max()\n",
    "        image = np.repeat(np.expand_dims(image, axis=2), 3, axis=2).astype(np.uint8)\n",
    "        image = Image.fromarray(image).resize((256, 256))\n",
    "        image = np.array(image)\n",
    "        images.append(image)\n",
    "    ptp_utils.view_images(np.concatenate(images, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_512(image_path, left=0, right=0, top=0, bottom=0):\n",
    "    if type(image_path) is str:\n",
    "        image = np.array(Image.open(image_path))[:, :, :3]\n",
    "    else:\n",
    "        image = image_path\n",
    "    h, w, c = image.shape\n",
    "    left = min(left, w-1)\n",
    "    right = min(right, w - left - 1)\n",
    "    top = min(top, h - left - 1)\n",
    "    bottom = min(bottom, h - top - 1)\n",
    "    image = image[top:h-bottom, left:w-right]\n",
    "    h, w, c = image.shape\n",
    "    if h < w:\n",
    "        offset = (w - h) // 2\n",
    "        image = image[:, offset:offset + h]\n",
    "    elif w < h:\n",
    "        offset = (h - w) // 2\n",
    "        image = image[offset:offset + w]\n",
    "    image = np.array(Image.fromarray(image).resize((512, 512)))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ERDDCIInversion:\n",
    "    \n",
    "    def prev_step(self, model_output: Union[torch.FloatTensor, np.ndarray], timestep: int, sample: Union[torch.FloatTensor, np.ndarray]):\n",
    "        prev_timestep = timestep - self.scheduler.config.num_train_timesteps // self.scheduler.num_inference_steps\n",
    "        alpha_prod_t = self.scheduler.alphas_cumprod[timestep]\n",
    "        alpha_prod_t_prev = self.scheduler.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else self.scheduler.final_alpha_cumprod\n",
    "        beta_prod_t = 1 - alpha_prod_t\n",
    "        pred_original_sample = (sample - beta_prod_t ** 0.5 * model_output) / alpha_prod_t ** 0.5\n",
    "        pred_sample_direction = (1 - alpha_prod_t_prev) ** 0.5 * model_output\n",
    "        prev_sample = alpha_prod_t_prev ** 0.5 * pred_original_sample + pred_sample_direction\n",
    "        return prev_sample\n",
    "    \n",
    "    def next_step(self, model_output: Union[torch.FloatTensor, np.ndarray], timestep: int, sample: Union[torch.FloatTensor, np.ndarray], ori_latent):\n",
    "        timestep, next_timestep = min(timestep - self.scheduler.config.num_train_timesteps // self.scheduler.num_inference_steps, 999), timestep\n",
    "        alpha_prod_t = self.scheduler.alphas_cumprod[timestep] if timestep >= 0 else self.scheduler.final_alpha_cumprod\n",
    "        alpha_prod_t_next = self.scheduler.alphas_cumprod[next_timestep]\n",
    "        beta_prod_t = 1 - alpha_prod_t\n",
    "        next_original_sample = (sample - beta_prod_t ** 0.5 * model_output) / alpha_prod_t ** 0.5\n",
    "        original_sample = ori_latent\n",
    "        next_sample_direction = (1 - alpha_prod_t_next) ** 0.5 * model_output\n",
    "        next_sample_direction_1 = (1 - alpha_prod_t_next) ** 0.5 * torch.randn_like(model_output)\n",
    "        next_sample = alpha_prod_t_next ** 0.5 * next_original_sample + next_sample_direction\n",
    "        return next_sample\n",
    "    \n",
    "    def get_noise_pred_single(self, latents, t, context):\n",
    "        noise_pred = self.model.unet(latents, t, encoder_hidden_states=context)[\"sample\"]\n",
    "        return noise_pred\n",
    "\n",
    "    # def get_noise_pred(self, latents, t, is_forward=True, context=None):\n",
    "    #     latents_input = torch.cat([latents] * 2)\n",
    "    #     if context is None:\n",
    "    #         context = self.context\n",
    "    #     guidance_scale = 1 if is_forward else GUIDANCE_SCALE\n",
    "    #     noise_pred = self.model.unet(latents_input, t, encoder_hidden_states=context)[\"sample\"]\n",
    "    #     noise_pred_uncond, noise_prediction_text = noise_pred.chunk(2)\n",
    "    #     noise_pred = noise_pred_uncond + guidance_scale * (noise_prediction_text - noise_pred_uncond)\n",
    "    #     if is_forward:\n",
    "    #         latents = self.next_step(noise_pred, t, latents)\n",
    "    #     else:\n",
    "    #         latents = self.prev_step(noise_pred, t, latents)\n",
    "    #     return latents\n",
    "    \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def latent2image(self, latents, return_type='np'):\n",
    "        latents = 1 / 0.18215 * latents.detach()\n",
    "        image = self.model.vae.decode(latents)['sample']\n",
    "        if return_type == 'np':\n",
    "            image = (image / 2 + 0.5).clamp(0, 1)\n",
    "            image = image.cpu().permute(0, 2, 3, 1).numpy()[0]\n",
    "            image = (image * 255).astype(np.uint8)\n",
    "        return image\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def image2latent(self, image):\n",
    "        with torch.no_grad():\n",
    "            if type(image) is Image:\n",
    "                image = np.array(image)\n",
    "            if type(image) is torch.Tensor and image.dim() == 4:\n",
    "                latents = image\n",
    "            else:\n",
    "                image = torch.from_numpy(image).float() / 127.5 - 1\n",
    "                image = image.permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "                latents = self.model.vae.encode(image)['latent_dist'].mean\n",
    "                latents = latents * 0.18215\n",
    "        return latents\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def init_prompt(self, prompt: str):\n",
    "        uncond_input = self.model.tokenizer(\n",
    "            [\"\"], padding=\"max_length\", max_length=self.model.tokenizer.model_max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        uncond_embeddings = self.model.text_encoder(uncond_input.input_ids.to(self.model.device))[0]\n",
    "        text_input = self.model.tokenizer(\n",
    "            [prompt],\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.model.tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        text_embeddings = self.model.text_encoder(text_input.input_ids.to(self.model.device))[0]\n",
    "        self.context = torch.cat([uncond_embeddings, text_embeddings])\n",
    "        self.prompt = prompt\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def ddim_loop(self, latent):\n",
    "        uncond_embeddings, cond_embeddings = self.context.chunk(2)\n",
    "        ori_latent = latent\n",
    "        aux_latent = latent\n",
    "        aux_all_latent = [aux_latent]\n",
    "        ori_all_latent = [ori_latent]\n",
    "        for i in range(NUM_DIFFUSION_STEPS):\n",
    "            t = self.model.scheduler.timesteps[len(self.model.scheduler.timesteps) - i - 1]\n",
    "            \n",
    "            noise_pred = self.get_noise_pred_single(ori_latent, t,  cond_embeddings)\n",
    "            ori_latent = self.next_step(noise_pred, t, t, ori_latent)\n",
    "            ori_all_latent.append(ori_latent)\n",
    "            \n",
    "            noise_pred = self.get_noise_pred_single(ori_latent, t,  cond_embeddings)\n",
    "            aux_latent = self.next_step(noise_pred, t, aux_latent)\n",
    "            aux_all_latent.append(aux_latent)\n",
    "        return aux_all_latent, ori_all_latent\n",
    "\n",
    "    @property\n",
    "    def scheduler(self):\n",
    "        return self.model.scheduler\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def ddim_inversion(self, image):\n",
    "        latent = self.image2latent(image)\n",
    "        image_rec = self.latent2image(latent)\n",
    "        aux_ddim_latents, ori_ddim_latents = self.ddim_loop(latent)\n",
    "        return image_rec, aux_ddim_latents, ori_ddim_latents\n",
    "\n",
    "    \n",
    "    def invert(self, image_path, prompt: str, offsets=(0,0,0,0), num_inner_steps=10, early_stop_epsilon=1e-5, verbose=False):\n",
    "        self.init_prompt(prompt)\n",
    "        ptp_utils.register_attention_control(self.model, None)\n",
    "        image_gt = load_512(image_path)\n",
    "        if verbose:\n",
    "            print(\"ERDDCI inversion...\")\n",
    "        print('image_gt',image_gt)\n",
    "        image_rec, aux_ddim_latents, ori_ddim_latents = self.ddim_inversion(image_gt)\n",
    "        return (image_gt, image_rec), aux_ddim_latents, ori_ddim_latents\n",
    "        \n",
    "    def __init__(self, model):\n",
    "        scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False,\n",
    "                                  set_alpha_to_one=False)\n",
    "        self.model = model\n",
    "        self.tokenizer = self.model.tokenizer\n",
    "        self.model.scheduler.set_timesteps(NUM_DIFFUSION_STEPS)\n",
    "        self.prompt = None\n",
    "        self.context = None\n",
    "\n",
    "erddci_inversion = ERDDCIInversion(ldm_stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: need rewrite\n",
    "def diffusion_step(model, controller, latents, context, t, guidance_scale, low_resource=False):\n",
    "    if low_resource:\n",
    "        noise_pred_uncond = model.unet(latents, t, encoder_hidden_states=context[0])[\"sample\"]\n",
    "        noise_prediction_text = model.unet(latents, t, encoder_hidden_states=context[1])[\"sample\"]\n",
    "    else:\n",
    "        latents_input = torch.cat([latents] * 2)\n",
    "        noise_pred = model.unet(latents_input, t, encoder_hidden_states=context)[\"sample\"]\n",
    "        noise_pred_uncond, noise_prediction_text = noise_pred.chunk(2)\n",
    "    noise_pred = noise_pred_uncond + guidance_scale * (noise_prediction_text - noise_pred_uncond)\n",
    "    latents = model.scheduler.step(noise_pred, t, latents)[\"prev_sample\"]\n",
    "    latents = controller.step_callback(latents)\n",
    "    return latents\n",
    "\n",
    "@torch.no_grad()\n",
    "def text2image_ldm_stable(\n",
    "    model,\n",
    "    prompt:  List[str],\n",
    "    controller,\n",
    "    num_inference_steps: int = 50,\n",
    "    guidance_scale: Optional[float] = 7.5,\n",
    "    generator: Optional[torch.Generator] = None,\n",
    "    latent: Optional[torch.FloatTensor] = None,\n",
    "    uncond_embeddings=None,\n",
    "    start_time=50,\n",
    "    return_type='image'\n",
    "):\n",
    "    batch_size = len(prompt)\n",
    "    ptp_utils.register_attention_control(model, controller)\n",
    "    height = width = 512\n",
    "    \n",
    "    text_input = model.tokenizer(\n",
    "        prompt,\n",
    "        padding=\"max_length\",\n",
    "        max_length=model.tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    text_embeddings = model.text_encoder(text_input.input_ids.to(model.device))[0]\n",
    "    max_length = text_input.input_ids.shape[-1]\n",
    "    if uncond_embeddings is None:\n",
    "        uncond_input = model.tokenizer(\n",
    "            [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "        uncond_embeddings_ = model.text_encoder(uncond_input.input_ids.to(model.device))[0]\n",
    "    else:\n",
    "        uncond_embeddings_ = None\n",
    "\n",
    "    latent, latents = ptp_utils.init_latent(latent, model, height, width, generator, batch_size)\n",
    "    model.scheduler.set_timesteps(num_inference_steps)\n",
    "    for i, t in enumerate(tqdm(model.scheduler.timesteps[-start_time:])):\n",
    "        if uncond_embeddings_ is None:\n",
    "            context = torch.cat([uncond_embeddings[i].expand(*text_embeddings.shape), text_embeddings])\n",
    "        else:\n",
    "            context = torch.cat([uncond_embeddings_, text_embeddings])\n",
    "        latents = ptp_utils.diffusion_step(model, controller, latents, context, t, guidance_scale, low_resource=False)\n",
    "        \n",
    "    if return_type == 'image':\n",
    "        image = ptp_utils.latent2image(model.vae, latents)\n",
    "    else:\n",
    "        image = latents\n",
    "    return image, latent\n",
    "\n",
    "def run_and_display(prompts, controller, latent=None, run_baseline=False, generator=None):\n",
    "    if run_baseline:\n",
    "        print(\"w.o. prompt-to-prompt\")\n",
    "        images, latent = run_and_display(prompts, EmptyControl(), latent=latent, run_baseline=False, generator=generator)\n",
    "        print(\"with prompt-to-prompt\")\n",
    "    images, x_t = ptp_utils.text2image_ldm_stable(ldm_stable, prompts, controller, latent=latent, num_inference_steps=NUM_DIFFUSION_STEPS, guidance_scale=3, generator=generator, low_resource=LOW_RESOURCE)\n",
    "    ptp_utils.view_images(images)\n",
    "    return images, x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_and_display_edit(prompts, controller, latent=None, latent_unt=None, guidance_scale=1.0, guidance_start_time = 1, run_baseline=False, generator=None):\n",
    "    if run_baseline:\n",
    "        print(\"w.o. prompt-to-prompt\")\n",
    "        images, latent = run_and_display_edit(prompts, EmptyControl(), latent=latent, latent_unt=latent_unt, \n",
    "                    guidance_scale=guidance_scale, guidance_start_time = guidance_start_time, run_baseline=False, generator=generator)\n",
    "        print(\"with prompt-to-prompt\")\n",
    "    images, x_t = ptp_utils.text2image_ldm_stable_edit(ldm_stable, prompts, controller, latent=latent, latent_unt=latent_unt, num_inference_steps=NUM_DIFFUSION_STEPS, guidance_scale=guidance_scale, guidance_start_time=guidance_start_time, generator=generator, low_resource=LOW_RESOURCE)\n",
    "    ptp_utils.view_images(images)\n",
    "    return images, x_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Cross-Attention Visualization\n",
    "First let's generate an image and visualize the cross-attention maps for each word in the prompt.\n",
    "Notice, we normalize each map to 0-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# 设置描述文件和图像文件夹的路径\n",
    "token_file_path = 'flickr30k_dataset/results_20130124.token'\n",
    "image_folder_path = 'flickr30k_dataset/flickr30k-images/'\n",
    "\n",
    "# 解析描述文件\n",
    "def parse_captions(file_path, target_caption_number=\"3\"):\n",
    "    captions_dict = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) < 2:  # 确保行内有足够的分割元素\n",
    "                continue\n",
    "            caption_id = parts[0]\n",
    "            caption_text = parts[1]\n",
    "            image_id, caption_num = caption_id.split('#')\n",
    "            if caption_num == target_caption_number:\n",
    "                captions_dict[image_id] = caption_text\n",
    "    return captions_dict\n",
    "\n",
    "# 加载图像并返回图像对象和描述\n",
    "def get_image_and_captions(image_path, captions_dict):\n",
    "    # 获取图像ID\n",
    "    image_id = os.path.basename(image_path).split('#')[0]\n",
    "    print(f\"Checking captions for image ID: {image_id}\")\n",
    "\n",
    "    # 加载图像\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "    except IOError:\n",
    "        print(f\"Error opening image {image_path}\")\n",
    "        return None, [\"Error opening image.\"]\n",
    "\n",
    "    # 获取图像描述\n",
    "    if image_id in captions_dict:\n",
    "        return img, image_id, captions_dict[image_id]\n",
    "    else:\n",
    "        return img, [\"No captions found for this image.\"]\n",
    "\n",
    "# 主程序\n",
    "if __name__ == \"__main__\":\n",
    "    # 解析描述文件\n",
    "    captions_dict = parse_captions(token_file_path)\n",
    "\n",
    "\n",
    "    # 设置变量k为第k张图像\n",
    "    k = 12447  # 例如，抽取第10张图像\n",
    "\n",
    "    # 获取图像文件列表\n",
    "    images = [img for img in os.listdir(image_folder_path) if img.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "    # 检查k是否在有效范围内\n",
    "    if k < 1 or k > len(images):\n",
    "        print(\"Error: 'k' is out of the valid range.\")\n",
    "    else:\n",
    "        # 抽取第k张图像\n",
    "        image_path = os.path.join(image_folder_path, '2833560457.jpg')  # 数组索引从0开始，所以k-1\n",
    "        print(\"qq:\",images[k-1])\n",
    "        image, image_id, captions = get_image_and_captions(image_path, captions_dict)\n",
    "        \n",
    "        # 在Jupyter Notebook中显示图像和描述\n",
    "        if image:\n",
    "            display(image)\n",
    "            print(captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def save_plot(image_data, filename):\n",
    "    plt.figure()\n",
    "    plt.imshow(image_data)\n",
    "    plt.axis('off')  # 不显示坐标轴\n",
    "    plt.savefig(filename, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_path = f\"flickr30k_dataset/flickr30k-images/{image_id}\"\n",
    "# prompt = captions\n",
    "image_path = \"./example_images/a2dcdb1105ffdbf66696d6465c6072c.jpg\"\n",
    "prompt =\"A blue SUV in the woods\"\n",
    "(image_gt, image_enc), ddim_latents, ori_ddim_latents = dc_inversion.invert(image_path, prompt, offsets=(0,0,200,0), verbose=True)\n",
    "save_plot(image_gt, 'images/image6_0.png')\n",
    "print(\"Modify or remove offsets according to your image!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_cpu = torch.Generator().manual_seed(8888)\n",
    "# prompts = [captions]\n",
    "prompts = [\"A blue SUV in the woods\"]\n",
    "controller = AttentionStore()\n",
    "image, _ = run_and_display_edit(prompts, controller, latent=ddim_latents[-1], latent_unt=ori_ddim_latents, guidance_scale=1.0, guidance_start_time = 0.0, run_baseline=False, generator=g_cpu)\n",
    "show_cross_attention(controller, res=16, from_where=(\"up\", \"down\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts = [prompt]\n",
    "# controller = AttentionStore()\n",
    "# image_inv, _ = run_and_display_edit(prompts, controller, latent=ori_ddim_latents[-1], latent_unt=ori_ddim_latents, guidance_scale=1.0, guidance_start_time = 0.0, run_baseline=False, generator=g_cpu)\n",
    "# show_cross_attention(controller, res=16, from_where=(\"up\", \"down\"))\n",
    "# print(\"showing from left to right: the ground truth image, the vq-autoencoder reconstruction, the null-text inverted image\")\n",
    "# ptp_utils.view_images([image_gt, image_enc, image_inv[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# g_cpu = torch.Generator().manual_seed(8888)\n",
    "# prompts = [\"a tiger sitting next to a mirror\"]\n",
    "# controller = AttentionStore()\n",
    "# image, x_t = run_and_display(prompts, controller, latent=ori_ddim_latents[-1], run_baseline=False, generator=g_cpu)\n",
    "# show_cross_attention(controller, res=16, from_where=(\"up\", \"down\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Replacement edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"A blue SUV in the woods\",\n",
    "           \"A blue SUV in the city\"]\n",
    "lb = LocalBlend(prompts, (\"woods\", \"city\"))\n",
    "controller = AttentionReplace(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.2, self_replace_steps=0.5, local_blend=lb)\n",
    "image_inv, _ = run_and_display_edit(prompts, controller, latent=ddim_latents[-1], latent_unt=ori_ddim_latents, guidance_scale=4.5, guidance_start_time = 0.1, run_baseline=False, generator=g_cpu)\n",
    "save_plot(image_inv[1], 'images/image6_4.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompts = [\"A dragonfly landed on a branch\",\n",
    "           \"A dragonfly landed on a flower\"]\n",
    "lb = LocalBlend(prompts, (\"branch\", \"flower\"))\n",
    "controller = AttentionReplace(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.3, self_replace_steps=0.4, local_blend=lb)\n",
    "image_inv, _ = run_and_display_edit(prompts, controller, latent=ddim_latents[-1], latent_unt=ori_ddim_latents, guidance_scale=6, guidance_start_time = 0.2, run_baseline=False, generator=g_cpu)\n",
    "# save_plot(image_inv[1], 'images/image2_1.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Modify Cross-Attention injection #steps for specific words\n",
    "Next, we can reduce the restriction on our lion by reducing the number of cross-attention injection with respect to the word \"lion\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompts = [\"a cat sitting next to a mirror\",\n",
    "           \"a tiger sitting next to a mirror\"]\n",
    "controller = AttentionReplace(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps={\"default_\": 1., \"tiger\": .8},\n",
    "                              self_replace_steps=0.6)\n",
    "image_inv, _ = run_and_display_edit(prompts, controller, latent=ddim_latents[-1], latent_unt=ori_ddim_latents, guidance_scale=8, guidance_start_time = 0.3, run_baseline=True, generator=g_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Local Edit\n",
    "Lastly, if we want to preseve the original burger, we can apply a local edit with respect to the squirrel and the lion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# prompts = [\"A painting of a squirrel eating a burger\",\n",
    "#            \"A painting of a lion eating a burger\"]\n",
    "# lb = LocalBlend(prompts, (\"squirrel\", \"lion\"))\n",
    "# controller = AttentionReplace(prompts, NUM_DIFFUSION_STEPS,\n",
    "#                               cross_replace_steps={\"default_\": 1., \"lion\": .4},\n",
    "#                               self_replace_steps=0.4, local_blend=lb)\n",
    "# _ = run_and_display(prompts, controller, latent=x_t, run_baseline=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# prompts = [\"A painting of a squirrel eating a burger\",\n",
    "#            \"A painting of a squirrel eating a lasagne\"]\n",
    "# lb = LocalBlend(prompts, (\"burger\", \"lasagne\"))\n",
    "# controller = AttentionReplace(prompts, NUM_DIFFUSION_STEPS,\n",
    "#                               cross_replace_steps={\"default_\": 1., \"lasagne\": .2},\n",
    "#                               self_replace_steps=0.4,\n",
    "#                               local_blend=lb)\n",
    "# _ = run_and_display(prompts, controller, latent=x_t, run_baseline=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Refinement edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompts = [\"A blue SUV in the woods\",\n",
    "           \"A blue  in the woods\"]\n",
    "\n",
    "lb = LocalBlend(prompts, (\"SUV\", \"SUV\"))\n",
    "# equalizer = get_equalizer(prompts[1], (\"watercolour\",), (5,))\n",
    "controller = AttentionRefine(prompts, NUM_DIFFUSION_STEPS,\n",
    "                             cross_replace_steps=0.2, \n",
    "                             self_replace_steps=.4, local_blend=lb)\n",
    "image_inv, _ = run_and_display_edit(prompts, controller, latent=ddim_latents[-1], latent_unt=ori_ddim_latents, guidance_scale=20, guidance_start_time = 0.2, run_baseline=False, generator=g_cpu)\n",
    "# save_plot(image_inv[1], 'images/image6_3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"a sitting cat .\",\n",
    "           \"a sitting sculptural cat .\"\n",
    "        ]\n",
    "\n",
    "lb = LocalBlend(prompts, (\"cat\", \"cat\"))\n",
    "# equalizer = get_equalizer(prompts[1], (\"sculptural\",), (5,))\n",
    "controller = AttentionRefine(prompts, NUM_DIFFUSION_STEPS,\n",
    "                             cross_replace_steps=.2, \n",
    "                             self_replace_steps=.2, local_blend=lb)\n",
    "image_inv, _ = run_and_display_edit(prompts, controller, latent=ddim_latents[-1], latent_unt=ori_ddim_latents, guidance_scale=14, guidance_start_time = 0.2, run_baseline=False, generator=g_cpu)\n",
    "save_plot(image_inv[1], 'images/image1_3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"A blue SUV in the autumn woods\"] * 2\n",
    "\n",
    "### pay 3 times more attention to the word \"smiling\"\n",
    "lb = LocalBlend(prompts, (\"woods\", \"woods\"))\n",
    "equalizer = get_equalizer(prompts[1], (\"autumn\",), (6,))\n",
    "controller = AttentionReweight(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.5,\n",
    "                               self_replace_steps=.6, local_blend=lb,\n",
    "                               equalizer=equalizer)\n",
    "image_inv, _ = run_and_display_edit(prompts, controller, latent=ddim_latents[-1], latent_unt=ori_ddim_latents, guidance_scale=6, guidance_start_time = 0.6, run_baseline=False, generator=g_cpu)\n",
    "save_plot(image_inv[1], 'images/image6_2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"a cat sitting next to a mirror\",\n",
    "           \"watercolor painting of a cat sitting next to a mirror\"\n",
    "        ]\n",
    "# lb = LocalBlend(prompts, (\"cat\", \"cat\"))\n",
    "# equalizer = get_equalizer(prompts[1], (\"watercolor\",), (5,))\n",
    "controller = AttentionRefine(prompts, NUM_DIFFUSION_STEPS,\n",
    "                             cross_replace_steps=.8, \n",
    "                             self_replace_steps=.4)\n",
    "image_inv, _ = run_and_display_edit(prompts, controller, latent=ddim_latents[-1], latent_unt=ori_ddim_latents, guidance_scale=12, guidance_start_time = 0.2, run_baseline=True, generator=g_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"a cat sitting next to a mirror\",\n",
    "           \"a sleeping cat sitting next to a mirror\"\n",
    "        ]\n",
    "lb = LocalBlend(prompts, (\"cat\", \"cat\"))\n",
    "controller = AttentionRefine(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8,\n",
    "                               self_replace_steps=.4, local_blend=lb)\n",
    "image_inv, _ = run_and_display_edit(prompts, controller, latent=ddim_latents[-1], latent_unt=ori_ddim_latents, guidance_scale=20, guidance_start_time = 0.3, run_baseline=True, generator=g_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_cpu = torch.Generator().manual_seed(8888)\n",
    "prompts = [\"A painting of a squirrel eating a burger\"]\n",
    "controller = AttentionStore()\n",
    "image, x_t = run_and_display(prompts, controller, latent=None, run_baseline=False, generator=g_cpu)\n",
    "show_cross_attention(controller, res=16, from_where=(\"up\", \"down\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts = [\"A painting of a squirrel eating a burger\",\n",
    "#            \"A painting of a lion eating a burger\"]\n",
    "prompts = [\"a sitting cat .\",\n",
    "           \"a sitting dog .\"]\n",
    "lb = LocalBlend(prompts, (\"cat\", \"dog\"))\n",
    "controller = AttentionReplace(prompts, NUM_DIFFUSION_STEPS,\n",
    "                              cross_replace_steps={\"default_\": 1., \"lion\": .4},\n",
    "                              self_replace_steps=0.4, local_blend=lb)\n",
    "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompts = [\"a photo of a house on a mountain\",\n",
    "           \"a photo of a house on a mountain at fall\"]\n",
    "\n",
    "\n",
    "controller = AttentionRefine(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8,\n",
    "                             self_replace_steps=.4)\n",
    "_ = run_and_display(prompts, controller, latent=x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompts = [\"a photo of a house on a mountain\",\n",
    "           \"a photo of a house on a mountain at winter\"]\n",
    "\n",
    "\n",
    "controller = AttentionRefine(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8,\n",
    "                             self_replace_steps=.4)\n",
    "_ = run_and_display(prompts, controller, latent=x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"soup\",\n",
    "           \"pea soup\"] \n",
    "\n",
    "lb = LocalBlend(prompts, (\"soup\", \"soup\"))\n",
    "\n",
    "controller = AttentionRefine(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8,\n",
    "                             self_replace_steps=.4,\n",
    "                             local_blend=lb)\n",
    "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Re-Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"a smiling bunny doll\"] * 2\n",
    "\n",
    "### pay 3 times more attention to the word \"smiling\"\n",
    "equalizer = get_equalizer(prompts[1], (\"smiling\",), (5,))\n",
    "controller = AttentionReweight(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8,\n",
    "                               self_replace_steps=.4,\n",
    "                               equalizer=equalizer)\n",
    "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"pink bear riding a bicycle\"] * 2\n",
    "\n",
    "### we don't wont pink bikes, only pink bear.\n",
    "### we reduce the amount of pink but apply it locally on the bikes (attention re-weight + local mask )\n",
    "\n",
    "### pay less attention to the word \"pink\"\n",
    "equalizer = get_equalizer(prompts[1], (\"pink\",), (-1,))\n",
    "\n",
    "### apply the edit on the bikes \n",
    "lb = LocalBlend(prompts, (\"bicycle\", \"bicycle\"))\n",
    "controller = AttentionReweight(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8,\n",
    "                               self_replace_steps=.4,\n",
    "                               equalizer=equalizer,\n",
    "                               local_blend=lb)\n",
    "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where are my croutons?\n",
    "It might be useful to use Attention Re-Weighting with a previous edit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"soup\",\n",
    "           \"pea soup with croutons\"] \n",
    "lb = LocalBlend(prompts, (\"soup\", \"soup\"))\n",
    "controller = AttentionRefine(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8,\n",
    "                             self_replace_steps=.4, local_blend=lb)\n",
    "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with more attetnion to `\"croutons\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"soup\",\n",
    "           \"pea soup with croutons\"] \n",
    "\n",
    "\n",
    "lb = LocalBlend(prompts, (\"soup\", \"soup\"))\n",
    "controller_a = AttentionRefine(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8, \n",
    "                               self_replace_steps=.4, local_blend=lb)\n",
    "\n",
    "### pay 3 times more attention to the word \"croutons\"\n",
    "equalizer = get_equalizer(prompts[1], (\"croutons\",), (3,))\n",
    "controller = AttentionReweight(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8,\n",
    "                               self_replace_steps=.4, equalizer=equalizer, local_blend=lb,\n",
    "                               controller=controller_a)\n",
    "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"potatos\",\n",
    "           \"fried potatos\"] \n",
    "lb = LocalBlend(prompts, (\"potatos\", \"potatos\"))\n",
    "controller = AttentionRefine(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8,\n",
    "                             self_replace_steps=.4, local_blend=lb)\n",
    "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"potatos\",\n",
    "           \"fried potatos\"] \n",
    "lb = LocalBlend(prompts, (\"potatos\", \"potatos\"))\n",
    "controller = AttentionRefine(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8, \n",
    "                             self_replace_steps=.4, local_blend=lb)\n",
    "\n",
    "### pay 10 times more attention to the word \"fried\"\n",
    "equalizer = get_equalizer(prompts[1], (\"fried\",), (10,))\n",
    "controller = AttentionReweight(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8,\n",
    "                               self_replace_steps=.4, equalizer=equalizer, local_blend=lb,\n",
    "                               controller=controller_a)\n",
    "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-11.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-11:m94"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
